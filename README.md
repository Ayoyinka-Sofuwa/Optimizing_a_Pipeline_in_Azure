# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This data is a Bank marketing data that contains information about its customers with 40 features and attributes about their personal information, financial, occupational and educational statuses, and the campaign details for each customer. We seek to use this data to predict if they will subscribe to a fixed term deposit in the bank or not.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was the Voting Ensemble for my AutoML run, with an accuracy of 0.91825 and AUC weighted of 0.94837.
For my hyperdrive configuration, my best run had an Accuracy(the primary metric) of 0.90759.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The classification experiment was run using the Logistic Regression algorithm, starting with the train.py script which data extracted from a webpath, and was cleaned using a clean_data function. The tabular dataset was created with the TabularDatasetFactory module and was divided into train and tests sets using the scikit learn's train_test_split module. I checked for errors in my script to prevent future errors in my experiment through the notebook in the studio.
Working with my newly created ComputeInstance to access the notebooks, I created an experiment Run with the workspace attributes then went on to check if there was any preregistered Compute cluster to be used for the experiment.
I created a compute cluster of vm_size, "standard ds_v2" and provisioned the maximum number of nodes to 4 with AmlCompute.
In my pipeline, I defined a range of values (search space) to be randomly selected, and find the best configuration for the best performance using the RandomParameterSampling parameters "C" and "max_iter", with smaller values to get a stronger regularization for the hyperparameter and the maximum number of iterations for the classification algorithm. And I used the "choice and uniform" expressions to tune the parameters. 
My termination policy was the "BanditPolicy" which had an evaluation interval of frequency 4, a slack factor of 0.1 and a delay evaluation policy starting from the 5th interval. I created a scikit-learn estimator to be used in the hyperdrive configuration by calling the script into the experiment from the directory, and the defining the compute target/cluster to be used. 
My hyperdrive configuration included the estimator, the policy, the parameter sampler, the primary metric as "Accuracy" and to maximize it as the goal, maximum concurrent runs, and the total runs for the experiment. I submitted this configuration for the hyperdrive and began the experiment run.

**What are the benefits of the parameter sampler you chose?**
The benefits of the parameter sampler is: the Random sampling influenced a faster run and the parameters search space allowed it to have a stronger regularization.

**What are the benefits of the early stopping policy you chose?**
The early stopping policy of evaluation interval, 4, and the slack factor of 0.1 allowed more room for more runs to be logged with the primary metric before applying the policy.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The model generated by the AutoML was the VotingEnsemble. The total experiment ran for 31m51.30s and had 51 iterations, the 51st iteration had the highest accuracy of 0.91825.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Differences
Considering the total number of runs in the two experiments, the AutoML experiment achieved more in a significantly lesser time: 51 runs in 31m51.30s and 15 runs in 14m13.68s.
In Accuracy, the AutoMl's best run's accuracy was 0.91825 and the hyperdrive's best run's accuracy was 0.9075873 for predicting whether the customers will make a fixed term deposit or not.
There was a difference in both experiments and AutoMl performed better because AutoML automatically trains models, is quicker, and generally performs with a better accuracy. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
For future work, and experiments: I would,
1. Reduce the evaluation interval for the policy so that it can quickly identify poorly performing runs early and terminate them and to also improve computational efficiency.
2. I'd increase the number of iterations in the max_iter parameter to allow for possible better accuracy
3. I'd also increase the number of concurrent runs inthe AutoMl to see if it'd have more iterations and much better accuracy.

## Proof of cluster clean up

